# Raspberry Pi Server
# armhf or arm32v7 --> 32bit
# arm64 or arm64v8--> 64bit

version: "3.7"

networks:
  rpi-net:
  rpi-lan:
    external: true

volumes:
  prometheus-data:
  grafana-data:

x-extra_hosts:
  # This is added so that within each service you can communicate to another service by using the DNS name of pi rather
  # than using IP address
  &rpi-server
  # Change this to IP of your Router
  - "router.home:192.168.1.1"
  # Change this to IP of your RPi
  - "server.home:192.168.1.100"
  - "plex.home:192.168.1.210"

services:

  # To run this container, you would need an openvpn provider. I use PIA and hence in here that is used as an
  # example. Please look at the documentation to understand each of the config variables.
  # Documentation: https://haugene.github.io/docker-transmission-openvpn/
  # Transmission: https://github.com/transmission/transmission/wiki/Editing-Configuration-Files
  transmission-openvpn:
    image: haugene/transmission-openvpn:3.0.2-arm64
    container_name: transmission-openvpn
    hostname: transmission
    restart: always
    ports:
      - 9091:9091
      - 9696:9696  # This is Prowlarr Port â€“ managed by VPN Service Network
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
      - OPENVPN_PROVIDER=${OPENVPN_PROVIDER}
      - OPENVPN_CONFIG=${OPENVPN_CONFIG}
      - OPENVPN_USERNAME=${OPENVPN_USERNAME}
      - OPENVPN_PASSWORD=${OPENVPN_PASSWORD}
      - OPENVPN_OPTS=--mute-replay-warnings --inactive 3600 --ping 10 --ping-exit 60
      - PIA_OPENVPN_CONFIG_BUNDLE=openvpn
      - LOCAL_NETWORK=${LOCAL_NETWORK}
      - DISABLE_PORT_UPDATER=${DISABLE_PORT_UPDATER}
      - TRANSMISSION_ALT_SPEED_DOWN=${TRANSMISSION_ALT_SPEED_DOWN}
      - TRANSMISSION_ALT_SPEED_TIME_DAY=${TRANSMISSION_ALT_SPEED_TIME_DAY}
      - TRANSMISSION_ALT_SPEED_TIME_ENABLED=${TRANSMISSION_ALT_SPEED_TIME_ENABLED}
      - TRANSMISSION_ALT_SPEED_UP=${TRANSMISSION_ALT_SPEED_UP}
      - TRANSMISSION_BLOCKLIST_ENABLED=${TRANSMISSION_BLOCKLIST_ENABLED}
      - TRANSMISSION_BLOCKLIST_URL=${TRANSMISSION_BLOCKLIST_URL}
      - TRANSMISSION_DOWNLOAD_DIR=${TRANSMISSION_DOWNLOAD_DIR}
      - TRANSMISSION_IDLE_SEEDING_LIMIT=${TRANSMISSION_IDLE_SEEDING_LIMIT}
      - TRANSMISSION_IDLE_SEEDING_LIMIT_ENABLED=${TRANSMISSION_IDLE_SEEDING_LIMIT_ENABLED}
      - TRANSMISSION_INCOMPLETE_DIR_ENABLED=${TRANSMISSION_INCOMPLETE_DIR_ENABLED}
      - TRANSMISSION_PEER_PORT_RANDOM_LOW=${TRANSMISSION_PEER_PORT_RANDOM_LOW}
      - TRANSMISSION_RATIO_LIMIT_ENABLED=${TRANSMISSION_RATIO_LIMIT_ENABLED}
      - TRANSMISSION_RPC_AUTHENTICATION_REQUIRED=${TRANSMISSION_RPC_AUTHENTICATION_REQUIRED}
      - TRANSMISSION_RPC_WHITELIST=${TRANSMISSION_RPC_WHITELIST}
      - TRANSMISSION_RPC_WHITELIST_ENABLED=${TRANSMISSION_RPC_WHITELIST_ENABLED}
      - TRANSMISSION_RPC_USERNAME=${TRANSMISSION_RPC_USERNAME}
      - TRANSMISSION_RPC_PASSWORD=${TRANSMISSION_RPC_PASSWORD}
      - TRANSMISSION_TRASH_ORIGINAL_TORRENT_FILES=${TRANSMISSION_TRASH_ORIGINAL_TORRENT_FILES}
      - TRANSMISSION_UMASK=${TRANSMISSION_UMASK}
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    privileged: true
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun # This creates a tunnel for Transmission
    volumes:
      - ${STORAGE}/Transmission/complete:/data/complete
      - ${STORAGE}/Transmission/incomplete:/data/incomplete
      - ./transmission:/data/transmission-home
  
  # Documentation: https://github.com/chriscrowe/docker-pihole-unbound/tree/main/one-container
  pihole:
    image: cbcrowe/pihole-unbound:2023.11.0
    container_name: pihole
    hostname: pihole
    restart: always
    ports:
      - 53:53/tcp
      - 53:53/udp
      - 443:443/tcp
      - 80:80/tcp
    environment:
      - TZ=${TZ}
      - WEBPASSWORD=${WEBPASSWORD}
      - FTLCONF_LOCAL_IPV4=${FTLCONF_LOCAL_IPV4}
      - REV_SERVER=${REV_SERVER}
      - REV_SERVER_DOMAIN=${REV_SERVER_DOMAIN}
      - REV_SERVER_TARGET=${REV_SERVER_TARGET}
      - REV_SERVER_CIDR=${REV_SERVER_CIDR}
      # If we don't specify two, it will auto pick google.
      - PIHOLE_DNS_1=127.0.0.1#5335       # Hardcoded to our Unbound server
      - PIHOLE_DNS_2=127.0.0.1#5335       # Hardcoded to our Unbound server
      - DNSSEC="true"                     # Enable DNSSEC
      - DNSMASQ_LISTENING="single"
      - DNS_BOGUS_PRIV="true"
    networks:
      rpi-net:
      rpi-lan:
        ipv4_address: 192.168.1.200
    extra_hosts: *rpi-server
    # Recommended but not required (DHCP needs NET_ADMIN)
    # https://github.com/pi-hole/docker-pi-hole#note-on-capabilities
    cap_add:
      - NET_ADMIN
    shm_size: '512mb'
    volumes:
      - ./pihole/etc-pihole/:/etc/pihole/
      - ./pihole/etc-dnsmasq.d/:/etc/dnsmasq.d/

  plex:
    image: lscr.io/linuxserver/plex:arm64v8-1.32.8
    container_name: plex
    hostname: plex
    restart: always
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
      - VERSION=docker
      - PLEX_CLAIM=${PLEX_CLAIM}
    extra_hosts: *rpi-server
    networks:
      rpi-net:
      rpi-lan:
        ipv4_address: 192.168.1.210
    volumes:
      - ./plex:/config
      - ${STORAGE}/Media/Series:/tv
      - ${STORAGE}/Media/Movies:/movies
    healthcheck:
      test: wget -nv -t 1 --spider --spider http://plex.home:32400/?X-Plex-Token=${PLEX_TOKEN} > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  # Documentation https://github.com/Taxel/PlexTraktSync
  plextraktsync:
    image: ghcr.io/taxel/plextraktsync:0.25.16
    container_name: plextraktsync
    restart: always
    depends_on: [plex]
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
    extra_hosts: *rpi-server
    networks:
      rpi-net:
      rpi-lan:
        ipv4_address: 192.168.1.220
    command: watch
    volumes:
      - ./plextraktsync:/app/config
    healthcheck:
      test: wget -nv -t 1 --spider http://plex.home:32400/?X-Plex-Token=${PLEX_TOKEN} > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  wireguard:
    image: linuxserver/wireguard:arm64v8-1.0.20210914
    container_name: wireguard
    hostname: wireguard
    restart: always
    depends_on: [pihole]
    ports:
      - 51820:51820/udp         # Forward port 51820/udp on your router to the server IP
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
      - SERVERURL=${DOMAIN}
      - SERVERPORT=51820
      - PEERS=2                 # How many peers to generate for you (clients)
      - PEERDNS=192.168.1.200   # Optional, put pihole if you have static IP
      - INTERNAL_SUBNET=10.10.10.0
      - ALLOWEDIPS=192.168.1.0/24
    extra_hosts: *rpi-server
    networks:
      rpi-net:
      rpi-lan:
        ipv4_address: 192.168.1.230
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    dns:
      - 192.168.1.200           # Put pihole if you have static IP
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv4.conf.all.src_valid_mark=1
    volumes:
      - ./wireguard:/config
      - /lib/modules:/lib/modules
    healthcheck:
      test: curl -o /dev/null -m 3 -fs https://1.1.1.1/dns-query?ct=application/dns-json&name=www.google.com&type=AAAA || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  sonarr:
    image: lscr.io/linuxserver/sonarr:arm64v8-3.0.10
    container_name: sonarr
    hostname: sonarr
    restart: always
    ports:
      - 8989:8989
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    volumes:
      - ./sonarr:/config
      - ${STORAGE}/Transmission/complete:/downloads
      - ${STORAGE}/Media/Series:/tv
    healthcheck:
      test: curl -fs http://localhost:8989 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  radarr:
    image: lscr.io/linuxserver/radarr:arm64v8-4.3.2
    container_name: radarr
    hostname: radarr
    restart: always
    ports:
      - 7878:7878
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    volumes:
      - ./radarr:/config
      - ${STORAGE}/Transmission/complete:/downloads
      - ${STORAGE}/Media/Movies:/movies
    healthcheck:
      test: curl -fs http://localhost:7878 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3
  
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:arm64v8-1.2.2
    container_name: prowlarr
    restart: always
    depends_on: [transmission-openvpn]
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
    network_mode: "service:transmission-openvpn"
    volumes:
      - ./prowlarr:/config
    healthcheck:
      test: curl -fs http://localhost:9696 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  overseerr:
    image: lscr.io/linuxserver/overseerr:arm64v8-1.33.2
    container_name: overseerr
    hostname: overseerr
    restart: always
    ports:
      - 5055:5055
    environment:
      - PUID=${UID}
      - PGID=${UID}
      - TZ=${TZ}
    extra_hosts: *rpi-server
    networks:
      rpi-net:
      rpi-lan:
        ipv4_address: 192.168.1.240
    volumes:
      - ./overseerr:/config
    healthcheck:
      test: curl -fs http://localhost:5055 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  navidrome:
    image: deluan/navidrome:pr-2840
    container_name: navidrome
    hostname: navidrome
    restart: always
    user: ${UID}:${UID}
    ports:
      - 4533:4533
    environment:
      - ND_SCANSCHEDULE=@every 1m
      - ND_LOGLEVEL=info  
      - ND_SESSIONTIMEOUT=24h
      # - ND_BASEURL=""
    volumes:
      - ./navidrome:/data
      - ${STORAGE}/Media/Music:/music:ro

  homer:
    image: b4bz/homer:v23.02.2
    container_name: homer
    restart: always
    user: "${UID}:${GID}"
    ports:
      - 8080:8080
    environment:
      - UID=${UID}
      - GID=${GID}
      - TZ=${TZ}
      - PASSWORD=${PASSWORD}
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    volumes:
      # - ./homer/custom.css:/www/assets/custom.css
      - ./homer/icons:/www/assets/icons
      - ./homer/config.yml:/www/assets/config.yml
    healthcheck:
      test: wget -nv -t 1 --spider --spider http://localhost:8080 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  nginx:
    image: jc21/nginx-proxy-manager:2.9.15
    container_name: nginx
    hostname: nginx
    restart: always
    ports:
      - 80:80
      - 81:81
      - 443:443
    environment:
      - UID=${UID}
      - GID=${GID}
      - DB_SQLITE_FILE=/data/database.sqlite
    networks:
      - rpi-net
    volumes:
      - ./nginx/config/config.json:/app/config/production.json
      - ./nginx/data:/data
      - ./nginx/letsencrypt:/etc/letsencrypt
    healthcheck:
      test: curl -fs http://localhost:81 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3
  
  portainer-ce:
    image: portainer/portainer-ce:linux-arm64-2.17.1
    container_name: portainer
    restart: always
    ports:
      - 8000:8000
      - 9000:9000
    environment:
      - UID=${UID}
      - GID=${GID}
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer/data:/data 

  samba:
    image: dperson/samba:aarch64
    container_name: samba
    restart: always
    ports:
      - 139:139
      - 445:445
    extra_hosts: *rpi-server
    networks:
      - rpi-net
    command: '-u ${SAMBA_USER};${SAMBA_PASS} -s "Drive;/Drive;yes;no"'
    stdin_open: true
    tty: true
    volumes:
      - /usr/share/zoneinfo/Europe/Madrid:/etc/localtime
      - /mnt/storage:/Drive

  duckdns:
    image: lscr.io/linuxserver/duckdns:arm64v8-version-ab5e2d0e
    container_name: duckdns
    restart: always
    environment:
      - PUID=${UID}
      - PGID=${GID}
      - TZ=${TZ}
      - SUBDOMAINS=${DOMAIN}
      - TOKEN=${TOKEN}
      - LOG_FILE=false
    networks:
      - rpi-net
    volumes:
      - ./duckdns:/config
    healthcheck:
      test: curl -o /dev/null -m 3 -fs https://1.1.1.1/dns-query?ct=application/dns-json&name=www.google.com&type=AAAA || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  ## Monitoring using cAdvisor, Node Exporter, Prometheus and Grafana 
  ## Documentation: https://github.com/oijkn/Docker-Raspberry-PI-Monitoring
  ## cAdvisor: https://github.com/ZCube/cadvisor-docker
  cadvisor:
    image: zcube/cadvisor:v0.45.0
    container_name: cadvisor
    hostname: cadvisor
    restart: always
    privileged: true
    expose:
      - 8080
    command:
      - --docker_only=true
      - --housekeeping_interval=30s
      - --disable_metrics=accelerator,cpu_topology,disk,tcp,udp,percpu,sched,process,hugetlb,resctrl,cpuset,advtcp
    devices:
      - /dev/kmsg
    networks:
      - rpi-net
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
      - /etc/machine-id:/etc/machine-id:ro
    healthcheck:
      test: wget -nv -t 1 --spider http://localhost:8080 > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  node-exporter:
    image: prom/node-exporter:v1.5.0
    container_name: node-exporter
    hostname: exporter
    restart: always
    expose:
      - 9100
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host
      - --collector.filesystem.ignored-mount-points
      - ^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)
    networks:
      - rpi-net
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /:/host:ro,rslave
    healthcheck:
      test: wget -nv -t 1 --spider http://localhost:9100 > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  prometheus:
    image: prom/prometheus:v2.43.0
    container_name: prometheus
    hostname: prometheus
    restart: always
    depends_on: [cadvisor, node-exporter]
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
    expose:
      - 9090
    networks:
      - rpi-net
    volumes:
      - prometheus-data:/prometheus
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:rw
    healthcheck:
      test: wget -nv -t 1 --spider http://localhost:9090 > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  grafana:
    image: grafana/grafana:9.4.0
    container_name: grafana
    hostname: grafana
    restart: always
    depends_on: [prometheus]
    user: "${UID}"
    ports:
      - 3000:3000
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_PATHS_CONFIG=${GF_PATHS_CONFIG}
      - GF_PATHS_DATA=${GF_PATHS_DATA}
      - GF_PATHS_HOME=${GF_PATHS_HOME}
      - GF_PATHS_LOGS=${GF_PATHS_LOGS}
      - GF_PATHS_PLUGINS=${GF_PATHS_PLUGINS}
      - GF_PATHS_PROVISIONING=${GF_PATHS_PROVISIONING}
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=${GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH}
    networks:
      - rpi-net
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    healthcheck:
      test: wget -nv -t 1 --spider http://localhost:3000/api/health > /dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3
